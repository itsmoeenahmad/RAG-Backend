# üöÄ Koyeb Timeout Fix - Production Implementation

## üéØ Problem Summary

**Issue**: 502/504 Gateway Timeout on Koyeb when uploading large PDFs (2-4 min processing time)

**Root Cause**: Cloudflare/Koyeb edge proxy timeout (100-120 seconds) terminates HTTP connection before FastAPI completes long-running document ingestion

**Impact**: Client sees timeout error despite successful backend processing

---

## ‚úÖ Solution Architecture

### Async Job Pattern with Status Tracking

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Client    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  POST /upload    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Returns job_id  ‚îÇ
‚îÇ  (Upload)   ‚îÇ     ‚îÇ  (Immediate)     ‚îÇ     ‚îÇ  Status: queued  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                      ‚îÇ
                                                      ‚îÇ Spawns
                                                      ‚ñº
                                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                            ‚îÇ Background Task  ‚îÇ
                                            ‚îÇ - Extract PDF    ‚îÇ
                                            ‚îÇ - Generate       ‚îÇ
                                            ‚îÇ   Embeddings     ‚îÇ
                                            ‚îÇ - Batch Upsert   ‚îÇ
                                            ‚îÇ   to Qdrant      ‚îÇ
                                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                      ‚îÇ
                                                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Client    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇGET /upload/status‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Returns:        ‚îÇ
‚îÇ  (Polling)  ‚îÇ     ‚îÇ  /{job_id}       ‚îÇ     ‚îÇ  - Progress %    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ  - Status        ‚îÇ
                                              ‚îÇ  - Chunks count  ‚îÇ
                                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîß Implementation Details

### 1. Job Tracking (MongoDB)

**New Collection**: `upload_jobs`

**Schema**:
```json
{
  "job_id": "uuid4-string",
  "user_id": "user123",
  "filename": "document.pdf",
  "status": "queued|processing|completed|failed",
  "collection_name": "user_user123_data",
  "inserted_chunks": 2255,
  "error_message": null,
  "progress_percent": 100,
  "created_at": "2026-01-31T12:00:00Z",
  "updated_at": "2026-01-31T12:03:45Z"
}
```

---

### 2. New API Endpoints

#### POST /upload (Async)

**Request**:
```bash
curl -X POST http://api.example.com/upload \
  -F "file=@large_document.pdf" \
  -F "user_id=user123"
```

**Response** (Immediate - ~200ms):
```json
{
  "job_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
  "user_id": "user123",
  "status": "queued",
  "message": "Upload queued successfully. Use GET /upload/status/{job_id} to check progress.",
  "created_at": "2026-01-31T12:00:00.123Z"
}
```

**Status Code**: 200 OK (returned instantly)

---

#### GET /upload/status/{job_id}

**Request**:
```bash
curl http://api.example.com/upload/status/a1b2c3d4-e5f6-7890-abcd-ef1234567890
```

**Response Examples**:

**Queued**:
```json
{
  "job_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
  "user_id": "user123",
  "status": "queued",
  "filename": "large_document.pdf",
  "progress_percent": 0,
  "created_at": "2026-01-31T12:00:00.123Z",
  "updated_at": "2026-01-31T12:00:00.123Z"
}
```

**Processing**:
```json
{
  "job_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
  "user_id": "user123",
  "status": "processing",
  "filename": "large_document.pdf",
  "progress_percent": 60,
  "created_at": "2026-01-31T12:00:00.123Z",
  "updated_at": "2026-01-31T12:01:30.456Z"
}
```

**Completed**:
```json
{
  "job_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
  "user_id": "user123",
  "status": "completed",
  "filename": "large_document.pdf",
  "collection_name": "user_user123_data",
  "inserted_chunks": 2255,
  "progress_percent": 100,
  "created_at": "2026-01-31T12:00:00.123Z",
  "updated_at": "2026-01-31T12:03:45.789Z"
}
```

**Failed**:
```json
{
  "job_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
  "user_id": "user123",
  "status": "failed",
  "filename": "large_document.pdf",
  "error_message": "Failed to connect to Qdrant: Connection timeout",
  "progress_percent": 30,
  "created_at": "2026-01-31T12:00:00.123Z",
  "updated_at": "2026-01-31T12:02:15.123Z"
}
```

---

### 3. Batched Qdrant Upserts

**Why**: Prevents memory exhaustion and API rate limits

**Implementation**:
- **Batch size**: 100 chunks per upsert
- **Benefits**:
  - Lower memory footprint
  - Better error recovery (partial failures)
  - Respects Qdrant API limits
  - Improves overall reliability

**Example**:
```python
# Before: Single upsert of 2255 chunks (risky)
vector_store.add_documents(documents)  # May fail/timeout

# After: 23 batches of 100 chunks each (robust)
vector_store.add_documents(documents, batch_size=100)
```

---

## üì± Client Implementation Guide

### JavaScript/TypeScript (React/Next.js)

```typescript
async function uploadPDF(file: File, userId: string) {
  // 1. Upload and get job_id
  const formData = new FormData();
  formData.append('file', file);
  formData.append('user_id', userId);
  
  const uploadRes = await fetch('https://api.example.com/upload', {
    method: 'POST',
    body: formData
  });
  
  const { job_id } = await uploadRes.json();
  console.log('Upload queued:', job_id);
  
  // 2. Poll for status
  return new Promise((resolve, reject) => {
    const pollInterval = setInterval(async () => {
      const statusRes = await fetch(`https://api.example.com/upload/status/${job_id}`);
      const status = await statusRes.json();
      
      console.log(`Progress: ${status.progress_percent}% - ${status.status}`);
      
      if (status.status === 'completed') {
        clearInterval(pollInterval);
        resolve(status);
      } else if (status.status === 'failed') {
        clearInterval(pollInterval);
        reject(new Error(status.error_message));
      }
    }, 3000); // Poll every 3 seconds
  });
}

// Usage
try {
  const result = await uploadPDF(pdfFile, 'user123');
  console.log(`‚úì Uploaded: ${result.inserted_chunks} chunks`);
} catch (error) {
  console.error('‚úó Upload failed:', error);
}
```

---

### Python Client

```python
import time
import requests

def upload_pdf(file_path: str, user_id: str, api_url: str):
    # 1. Upload
    with open(file_path, 'rb') as f:
        response = requests.post(
            f"{api_url}/upload",
            files={"file": f},
            data={"user_id": user_id}
        )
    
    job = response.json()
    job_id = job['job_id']
    print(f"Upload queued: {job_id}")
    
    # 2. Poll for completion
    while True:
        status_response = requests.get(f"{api_url}/upload/status/{job_id}")
        status = status_response.json()
        
        print(f"Progress: {status['progress_percent']}% - {status['status']}")
        
        if status['status'] == 'completed':
            print(f"‚úì Completed: {status['inserted_chunks']} chunks")
            return status
        elif status['status'] == 'failed':
            raise Exception(f"Upload failed: {status['error_message']}")
        
        time.sleep(3)  # Poll every 3 seconds

# Usage
result = upload_pdf("large_doc.pdf", "user123", "https://api.example.com")
```

---

### Flutter/Dart (Mobile)

```dart
Future<Map<String, dynamic>> uploadPDF(File file, String userId) async {
  // 1. Upload
  var request = http.MultipartRequest(
    'POST',
    Uri.parse('https://api.example.com/upload'),
  );
  
  request.files.add(await http.MultipartFile.fromPath('file', file.path));
  request.fields['user_id'] = userId;
  
  var uploadResponse = await request.send();
  var uploadData = jsonDecode(await uploadResponse.stream.bytesToString());
  String jobId = uploadData['job_id'];
  
  print('Upload queued: $jobId');
  
  // 2. Poll for status
  while (true) {
    await Future.delayed(Duration(seconds: 3));
    
    var statusResponse = await http.get(
      Uri.parse('https://api.example.com/upload/status/$jobId'),
    );
    
    var status = jsonDecode(statusResponse.body);
    print('Progress: ${status['progress_percent']}% - ${status['status']}');
    
    if (status['status'] == 'completed') {
      print('‚úì Completed: ${status['inserted_chunks']} chunks');
      return status;
    } else if (status['status'] == 'failed') {
      throw Exception('Upload failed: ${status['error_message']}');
    }
  }
}
```

---

## üé® Recommended UI/UX Pattern

### Upload Flow

1. **User clicks "Upload"**
   - Show loading spinner: "Uploading..."
   - POST /upload ‚Üí Get job_id

2. **Job queued**
   - Show progress UI: "Processing document... 0%"
   - Start polling GET /upload/status/{job_id}

3. **Processing updates**
   - Update progress bar: 10% ‚Üí 30% ‚Üí 60% ‚Üí 90%
   - Show message: "Extracting text... Generating embeddings... Indexing..."

4. **Completion**
   - Show success: "‚úì Document uploaded successfully! 2,255 chunks indexed"
   - Enable chat interface

5. **Error handling**
   - Show error: "‚úó Upload failed: [error_message]"
   - Offer retry button

---

## ‚öôÔ∏è Configuration Recommendations

### Polling Strategy

```javascript
// Adaptive polling (recommended)
const pollInterval = (attempt) => {
  if (attempt < 5) return 2000;      // First 10s: poll every 2s
  if (attempt < 15) return 5000;     // Next 50s: poll every 5s
  return 10000;                      // After 1min: poll every 10s
};
```

### Timeout Handling

```javascript
const MAX_POLL_TIME = 600000; // 10 minutes max
const startTime = Date.now();

const poll = async () => {
  if (Date.now() - startTime > MAX_POLL_TIME) {
    throw new Error('Upload timeout after 10 minutes');
  }
  // ... polling logic
};
```

---

## üîç Monitoring & Debugging

### Backend Logs to Watch

```bash
# Successful flow
INFO: Upload job a1b2c3d4 queued for user: user123
INFO: Starting background processing for job: a1b2c3d4
INFO: Extracting text from PDF: document.pdf
INFO: Processing 2255 chunks in batches of 100
INFO: Upserting batch 1-100/2255 into Qdrant
INFO: Upserting batch 101-200/2255 into Qdrant
...
INFO: Successfully inserted 2255 total chunks
INFO: Job a1b2c3d4 completed
```

### MongoDB Queries for Debugging

```javascript
// Check all jobs for a user
db.upload_jobs.find({ user_id: "user123" }).sort({ created_at: -1 })

// Find failed jobs
db.upload_jobs.find({ status: "failed" })

// Check stuck jobs (processing > 10 min)
db.upload_jobs.find({
  status: "processing",
  updated_at: { $lt: new Date(Date.now() - 10 * 60 * 1000) }
})
```

---

## üìä Performance Metrics

### Expected Timings

| Document Size | Chunks | Processing Time | Edge Timeout Risk |
|---------------|--------|-----------------|-------------------|
| Small (1-5 pages) | 50-200 | 20-40s | ‚úì Safe (old sync approach) |
| Medium (10-30 pages) | 400-1000 | 60-120s | ‚ö†Ô∏è Risky (timeouts possible) |
| Large (50+ pages) | 1500-3000 | 180-300s | ‚úó Always times out (sync) |

**With new async pattern**: All sizes complete successfully ‚úì

---

## üõ°Ô∏è Error Handling Best Practices

### Backend Resilience

```python
# Automatic retries for transient failures
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
def upsert_batch_to_qdrant(batch):
    return vector_store.add_documents(batch)
```

### Client Resilience

```typescript
// Exponential backoff for network errors
async function pollWithRetry(jobId: string, maxRetries = 5) {
  for (let i = 0; i < maxRetries; i++) {
    try {
      const response = await fetch(`/upload/status/${jobId}`);
      return await response.json();
    } catch (error) {
      if (i === maxRetries - 1) throw error;
      await sleep(Math.pow(2, i) * 1000); // 1s, 2s, 4s, 8s, 16s
    }
  }
}
```

---

## üöÄ Deployment Checklist

### Before Deploying

- [ ] Test with small document (verify immediate response)
- [ ] Test with large document (verify background processing)
- [ ] Verify MongoDB `upload_jobs` collection exists
- [ ] Check Qdrant connection pooling settings
- [ ] Ensure temp file cleanup works

### After Deploying

- [ ] Monitor first real uploads in logs
- [ ] Verify job status polling works from client
- [ ] Check for orphaned temp files
- [ ] Monitor MongoDB storage growth
- [ ] Set up job cleanup cron (delete old completed jobs)

---

## üîÑ Migration Strategy

### For Existing Clients

**Option 1: Dual endpoints (recommended)**
- Keep old `/upload` as deprecated
- Add new `/upload-async` endpoint
- Gradually migrate clients

**Option 2: Detect document size**
```python
@router.post("/upload")
async def upload_smart(file: UploadFile, ...):
    file_size = await get_file_size(file)
    
    if file_size < 5_000_000:  # 5MB threshold
        # Process synchronously (old way)
        return process_sync(file, user_id)
    else:
        # Process async (new way)
        return process_async(file, user_id)
```

---

## üéØ Summary

### What Changed

| Component | Before | After |
|-----------|--------|-------|
| **Upload endpoint** | Blocking (2-4 min) | Async (200ms) |
| **Response** | Chunks count | job_id + status |
| **Client flow** | Single request | Upload ‚Üí Poll status |
| **Qdrant upserts** | All at once | Batched (100 each) |
| **Timeout risk** | ‚úó High | ‚úì Eliminated |
| **Job tracking** | None | MongoDB collection |

### Key Benefits

‚úÖ **No more 502/504 errors**
‚úÖ **Better user experience** (progress tracking)
‚úÖ **More robust** (batched processing, error recovery)
‚úÖ **Scalable** (handles any document size)
‚úÖ **Observable** (job status, progress, errors)

---

## üìû Support

If you encounter issues:

1. Check backend logs for job_id
2. Query MongoDB `upload_jobs` collection
3. Verify Qdrant connectivity
4. Check temp file permissions

**Common Issues**:
- **Job stuck in "queued"**: Background task not running
- **Job stuck in "processing"**: Qdrant connection timeout
- **"Job not found"**: job_id mismatch or MongoDB connection issue

---

**Document Version**: 1.0
**Last Updated**: January 31, 2026
**Implementation Status**: ‚úÖ Production Ready
